# Translator Service - Full Build from Source for ARM64 + CUDA
# This builds PyTorch, fairseq2, and seamless_communication from source
# Build time: ~2-4 hours depending on hardware
#
# Build: docker build -f Dockerfile.build -t inference/translator-build:local .
# Run:   docker run --gpus all -p 7104:7104 inference/translator-build:local

# Stage 1: Build PyTorch from source with CUDA support
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS pytorch-builder

ENV DEBIAN_FRONTEND=noninteractive
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0"
ENV USE_CUDA=1
ENV USE_CUDNN=1
ENV USE_MKLDNN=1
ENV MAX_JOBS=8

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    ca-certificates \
    ccache \
    cmake \
    curl \
    git \
    libjpeg-dev \
    libpng-dev \
    libsndfile1-dev \
    ninja-build \
    python3 \
    python3-dev \
    python3-pip \
    python3-venv \
    && rm -rf /var/lib/apt/lists/*

# Create venv and install build dependencies
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

RUN pip install --no-cache-dir \
    numpy \
    pyyaml \
    setuptools \
    cmake \
    cffi \
    typing_extensions \
    future \
    six \
    requests \
    dataclasses \
    filelock \
    sympy \
    networkx \
    jinja2 \
    fsspec

# Clone and build PyTorch 2.1.0 (compatible with fairseq2 0.2.x)
WORKDIR /build
RUN git clone --recursive --branch v2.1.0 --depth 1 https://github.com/pytorch/pytorch.git

WORKDIR /build/pytorch
RUN pip install -r requirements.txt
RUN python setup.py bdist_wheel

# Stage 2: Build fairseq2 from source
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS fairseq2-builder

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    ca-certificates \
    cmake \
    git \
    libsndfile1-dev \
    ninja-build \
    python3 \
    python3-dev \
    python3-pip \
    python3-venv \
    && rm -rf /var/lib/apt/lists/*

# Create venv
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy PyTorch wheel from previous stage
COPY --from=pytorch-builder /build/pytorch/dist/*.whl /tmp/
RUN pip install /tmp/*.whl

# Install fairseq2 build dependencies
RUN pip install --no-cache-dir \
    cmake \
    ninja \
    numpy \
    packaging \
    setuptools \
    wheel

# Clone fairseq2 v0.2.0
WORKDIR /build
RUN git clone --recursive --branch v0.2.0 --depth 1 https://github.com/facebookresearch/fairseq2.git

# Build fairseq2n
WORKDIR /build/fairseq2/fairseq2n
RUN cmake -GNinja -DFAIRSEQ2N_USE_CUDA=ON -B build
RUN cmake --build build

# Install fairseq2n and fairseq2
WORKDIR /build/fairseq2/fairseq2n/python
RUN pip install -e . --no-build-isolation

WORKDIR /build/fairseq2
RUN pip install -e . --no-build-isolation

# Install seamless_communication
RUN pip install --no-deps "seamless_communication @ git+https://github.com/facebookresearch/seamless_communication.git"
RUN pip install --no-cache-dir \
    datasets==2.18.0 \
    fire \
    librosa \
    scipy \
    soundfile \
    torchaudio \
    sonar-space==0.2.1 \
    simuleval~=1.1.3

# Stage 3: Runtime image
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS runtime

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    libsndfile1 \
    python3 \
    python3-pip \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Copy venv from builder
COPY --from=fairseq2-builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy fairseq2 source (needed for editable install)
COPY --from=fairseq2-builder /build/fairseq2 /opt/fairseq2

WORKDIR /app

# Copy backend code
COPY backend/ /app/backend/

# Install backend dependencies
RUN pip install --no-cache-dir \
    fastapi \
    uvicorn \
    pydantic \
    loguru \
    python-dotenv \
    pydub

# Environment defaults
ENV API_HOST=0.0.0.0
ENV API_PORT=8104
ENV PYTHONPATH=/app/backend

EXPOSE 8104

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python3 -c "import urllib.request; urllib.request.urlopen('http://localhost:8104/health')" || exit 1

CMD ["python3", "-m", "uvicorn", "seamless_api.main:app", "--host", "0.0.0.0", "--port", "8104"]
