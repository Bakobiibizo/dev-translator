services:
  dev-translator:
    build:
      context: .
      dockerfile: docker/Dockerfile.core
      args:
        UID: ${UID:-1000}
        GID: ${GID:-1000}
    image: ${CORE_IMAGE:-devkit-core:local}
    working_dir: /workspace
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    stdin_open: true
    tty: true
    volumes:
      - models:/models
      - hf-cache:/home/dev/.cache/huggingface
      - torch-cache:/home/dev/.cache/torch
      - uv-cache:/home/dev/.cache/uv
      - .:/workspace:cached
    ports:
      - "${API_PORT:-8000}:${API_PORT:-8000}"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HOME=/home/dev
      - HF_HOME=/home/dev/.cache/huggingface
      - TRANSFORMERS_CACHE=/home/dev/.cache/huggingface
      - TORCH_HOME=/home/dev/.cache/torch
      - API_HOST=0.0.0.0
      - API_PORT=${API_PORT:-8000}
      - BACKEND_PORT=${BACKEND_PORT:-8104}
      - BACKEND_URL=${BACKEND_URL:-http://localhost:${BACKEND_PORT:-8104}}
      # Backend process (Python) config
      - BACKEND_CMD=/bin/bash
      - BACKEND_ARGS=-lc cd /workspace/backend && HOST=0.0.0.0 PORT=${BACKEND_PORT:-8104} python3 -m seamless_api.main
      - BACKEND_WORKDIR=/workspace/backend
      - BACKEND_HEALTH_PATH=/health
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    user: "${UID:-1000}:${GID:-1000}"
    command: ["bash", "-lc", "cd /workspace/backend && HOST=0.0.0.0 PORT=${BACKEND_PORT:-8104} python3 -m seamless_api.main & sleep 5 && cd /workspace && cargo run --bin dev-translator"]

volumes:
  models:
  hf-cache:
  torch-cache:
  uv-cache:
